{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsOPnBBCBBUm",
        "outputId": "299dbeb9-a91a-4403-f048-bae735bfa36b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import itertools\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.models import resnet34\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metadata_path = \"drive/MyDrive/train_data.csv\"\n",
        "metadata = pd.read_csv(metadata_path)\n",
        "test_data_path=\"drive/MyDrive/Test_data.csv\"\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "train_dir=\"drive/MyDrive/Colab Notebooks/MyTrain\"\n",
        "test_dir=\"drive/MyDrive/newimg/myTest\"\n",
        "\n",
        "model_path = \"drive/MyDrive/model.pth\"\n",
        "\n",
        "def random_split(dataset, train_ratio=0.8, shuffle=True, random_seed=None):\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "    indices = list(range(len(dataset)))\n",
        "    if shuffle:\n",
        "        random.shuffle(indices)\n",
        "    split = int(train_ratio * len(dataset))\n",
        "    train_indices = indices[:split]\n",
        "    val_indices = indices[split:]\n",
        "    return train_indices, val_indices\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LyricsDataset(Dataset):\n",
        "    def __init__(self, filename, train_dir, img_path):\n",
        "        self.data = filename\n",
        "        self.num_of_labels = len(self.unique_labels())\n",
        "        self.labels = self.enumerate_labels()\n",
        "        self.artists_list = list(self.labels.keys())\n",
        "        self.norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.train_dir = train_dir\n",
        "        if img_path != \"new_filename\":\n",
        "          self.small_data_set()\n",
        "        self.img_path = img_path\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_path = self.data.iloc[idx][self.img_path]\n",
        "        artistAnchor = self.data.iloc[idx]['artist']\n",
        "\n",
        "        different_artist = random.choice(list(self.labels.keys()))\n",
        "        while different_artist == artistAnchor:\n",
        "            different_artist = random.choice(list(self.labels.keys()))\n",
        "        negative = random.choice(self.data[self.data['artist'] == different_artist][self.img_path].values)\n",
        "\n",
        "        positive = img1_path\n",
        "        while positive == img1_path:\n",
        "            positive = random.choice(self.data[self.data['artist'] == artistAnchor][self.img_path].values)\n",
        "\n",
        "        anchor = Image.open(os.path.join(self.train_dir, img1_path))\n",
        "        negative = Image.open(os.path.join(self.train_dir, negative))\n",
        "        positive = Image.open(os.path.join(self.train_dir, positive))\n",
        "\n",
        "        anchor = np.array(self.norm(self.to_tensor(anchor)))\n",
        "        negative = np.array(self.norm(self.to_tensor(negative)))\n",
        "        positive = np.array(self.norm(self.to_tensor(positive)))\n",
        "\n",
        "        return anchor, positive, negative\n",
        "\n",
        "    def enumerate_labels(self):\n",
        "        c = 0\n",
        "        d = {}\n",
        "        labels = self.unique_labels()\n",
        "        for artist_name in labels:\n",
        "            d[artist_name] = c\n",
        "            c += 1\n",
        "        return d\n",
        "\n",
        "    def unique_labels(self):\n",
        "        labels = self.data['artist'].dropna().tolist()\n",
        "        return set(labels)\n",
        "\n",
        "    def print_stats(self):\n",
        "        print(\"#######################\")\n",
        "        print(\"dataset name: \", self.name)\n",
        "        print(\"dataset length: \", len(self.data))\n",
        "        print(\"number of labels: \", len(self.unique_labels()))\n",
        "        print(\"number of words in all lyrics: \", sum(len(x) for x in self.data['text'].to_list()))\n",
        "        print(\"#######################\")\n",
        "    def small_data_set(self):\n",
        "      labels = self.data['artist'].dropna().tolist()\n",
        "      hist = {}\n",
        "      m=[]\n",
        "      for label in labels:\n",
        "          if label in hist:\n",
        "              hist[label] += 1\n",
        "          else:\n",
        "              hist[label] = 1\n",
        "\n",
        "      labels = []\n",
        "      for label, count in hist.items():\n",
        "          if count >= 3:\n",
        "              labels.append(label)\n",
        "\n",
        "      def filter_rows(group):\n",
        "          return group.head(3)\n",
        "\n",
        "      self.data = self.data[self.data['artist'].isin(labels)].groupby('artist', group_keys=False).apply(filter_rows).head(5000)\n",
        "\n",
        "\n",
        "\n",
        "class EnhancedSiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedSiameseNetwork, self).__init__()\n",
        "        # Use ResNet18 as the backbone\n",
        "        self.backbone = models.resnet18(pretrained=True)\n",
        "        self.backbone.fc = nn.Sequential(\n",
        "            nn.Linear(self.backbone.fc.in_features, 512),\n",
        "            nn.Linear(512, 256)  # Change the output size to 256\n",
        "        )\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        # Forward pass for one branch of the Siamese network\n",
        "        return self.backbone(x)\n",
        "\n",
        "    def forward(self, input1, input2, input3):\n",
        "\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "        output3 = self.forward_once(input3)\n",
        "        return output1, output2, output3\n",
        "\n",
        "\n",
        "class TripletLoss(torch.nn.Module):\n",
        "    def __init__(self, margin=5.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        distance_positive = F.pairwise_distance(anchor, positive, keepdim=True)\n",
        "        distance_negative = F.pairwise_distance(anchor, negative, keepdim=True)\n",
        "        loss = torch.mean(torch.clamp(distance_positive - distance_negative + self.margin, min=0.0))\n",
        "        return loss\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model instantiation with the enhanced model\n",
        "model = EnhancedSiameseNetwork().to(device)\n",
        "criterion = TripletLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.00001)\n",
        "scheduler = StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# DataLoaders\n",
        "\n",
        "# train_dataset = LyricsDataset(metadata, train_dir, 'filename')\n",
        "test_dataset = LyricsDataset(test_data, test_dir, 'new_filename')\n",
        "\n",
        "# train_indices, val_indices = random_split(train_dataset, train_ratio=0.8, random_seed=42)\n",
        "\n",
        "    # Create DataLoader for training subset\n",
        "# train_subset = Subset(train_dataset, train_indices)\n",
        "# train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
        "\n",
        "#     # Create DataLoader for validation subset\n",
        "# val_subset = Subset(train_dataset, val_indices)\n",
        "# val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
        "\n",
        "# train_dataset = SiameseNetworkDataset(train_pairs, train_labels, train_dir)\n",
        "# test_dataset = SiameseNetworkDataset(test_pairs, test_labels, test_dir)\n",
        "\n",
        "#train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# for i,j,g in train_loader:\n",
        "#   pass\n",
        "for i,j,g in test_loader:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# def train_epoch(model, device, train_loader, criterion, optimizer):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     for img1, img2, img3 in train_loader:\n",
        "#         img1, img2, img3 = img1.float().to(device), img2.float().to(device), img3.float().to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         output1, output2, output3 = model(img1, img2, img3)\n",
        "#         loss = criterion(output1, output2, output3)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         # Compute training accuracy\n",
        "#         positive = F.pairwise_distance(output1, output2, keepdim=True)\n",
        "#         negative = F.pairwise_distance(output1, output3, keepdim=True)\n",
        "#         predictions = positive < negative\n",
        "#         labels = torch.ones_like(predictions).bool()\n",
        "#         correct += (predictions == labels).sum().item()\n",
        "#         total += labels.size(0)\n",
        "\n",
        "#     avg_loss = total_loss / len(train_loader)\n",
        "#     accuracy = correct / total\n",
        "#     print(f\"Training Loss: {avg_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
        "#     return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, img3 in test_loader:\n",
        "            img1, img2, img3 = img1.float().to(device), img2.float().to(device), img3.float().to(device)\n",
        "            output1, output2, output3 = model(img1, img2, img3)\n",
        "            test_loss += criterion(output1, output2, output3).item()\n",
        "            positive = F.pairwise_distance(output1, output2, keepdim=True)\n",
        "            negative = F.pairwise_distance(output1, output3, keepdim=True)\n",
        "            predictions = positive < negative\n",
        "            labels = torch.ones_like(predictions).bool()\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    return test_loss, accuracy\n",
        "\n",
        "# Main Training and Testing Loop\n",
        "\n",
        "\n",
        "    # train_loss, train_accuracy = train_epoch(model, device, train_loader, criterion, optimizer)\n",
        "    # test_loss, test_accuracy = evaluate_model(model, device, val_loader, criterion,0)\n",
        "num1,num2 = evaluate_model(model, device, test_loader, criterion)\n",
        "scheduler.step()\n",
        "\n",
        "    # train_losses.append(train_loss)\n",
        "    # train_accuracies.append(train_accuracy)\n",
        "    # test_losses.append(test_loss)\n",
        "    # test_accuracies.append(test_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Plotting\n",
        "# plt.figure(figsize=(10, 5))\n",
        "\n",
        "# # Plotting training and test losses\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
        "# plt.plot(range(1, epochs + 1), test_losses, label='Val Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.title('Training and Val Losses')\n",
        "# plt.legend()\n",
        "\n",
        "# # Plotting training and test accuracies\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy')\n",
        "# plt.plot(range(1, epochs + 1), test_accuracies, label='Val Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.title('Training and Val Accuracies')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2OCtDU8BCVs",
        "outputId": "aa5d2a99-05c5-469e-aaaf-b2938eed4fd6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7878\n"
          ]
        }
      ]
    }
  ]
}